{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5abf01ec",
   "metadata": {},
   "source": [
    "# Classifying between Imagination and Perception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02677981",
   "metadata": {},
   "source": [
    "For each task, orthographic, pictorial or audio, we classify to see whether the subject is imagining or perceiving the stimulus. The task selection should be done in the batch_mill main script. An output csv file is saved with the classification results. In this example, SVM is implemented. However, there is the option to implement CSP, PCA and XGBoost which are currently commented out in the classification section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f15b8f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mne.decoding import CSP\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "#from scipy import integrate, stats\n",
    "#import antropy as ant\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a5b4d2",
   "metadata": {},
   "source": [
    "#### Select parameters\n",
    "Either these can be batch run from the batch-papermill script, or chosen here and run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56409f33",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "subject = '12'\n",
    "session = '1'\n",
    "task = 'pictorial' \n",
    "iterations = 50 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3b42e",
   "metadata": {},
   "source": [
    "### Here the task selection is used to set other parameters\n",
    "The duration is the length of the perception trial. The tag is used to identify epochs based on their event_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3531425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == 'pictorial':\n",
    "    tag = 'p'\n",
    "    duration = 3\n",
    "elif task == 'orthographic':\n",
    "    print(\"orthographic decoding task\")\n",
    "    tag='t'\n",
    "    duration = 3\n",
    "elif task == 'audio':\n",
    "    tag='s'\n",
    "    duration = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4666af4",
   "metadata": {},
   "source": [
    "### Loading up the data\n",
    "To make the perception and imagination trials equal, we crop the imagination epochs to be of equal length to the perception duration. This is 2 seconds for audio and 3 seconds for the two visual modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c58c956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading X:\\CompSci\\ResearchProjects\\EJONeill\\Neuroimaging\\multisensoryeeg\\processed_eeg\\epochs\\perception_pictorial\\12_1_epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    3000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "109 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlw69\\AppData\\Local\\Temp\\ipykernel_15448\\1644791318.py:11: RuntimeWarning: This filename (X:\\CompSci\\ResearchProjects\\EJONeill\\Neuroimaging\\multisensoryeeg\\processed_eeg\\epochs\\perception_pictorial\\12_1_epo.fif) does not conform to MNE naming conventions. All events files should end with .eve, -eve.fif, -eve.fif.gz, -eve.lst, -eve.txt, _eve.fif, _eve.fif.gz, _eve.lst, _eve.txt or -annot.fif\n",
      "  perception_events = mne.read_events(perception_path+ datapoint)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading X:\\CompSci\\ResearchProjects\\EJONeill\\Neuroimaging\\multisensoryeeg\\processed_eeg\\epochs\\imagine_pictorial\\12_1_epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    4000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "111 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlw69\\AppData\\Local\\Temp\\ipykernel_15448\\1644791318.py:15: RuntimeWarning: This filename (X:\\CompSci\\ResearchProjects\\EJONeill\\Neuroimaging\\multisensoryeeg\\processed_eeg\\epochs\\imagine_pictorial\\12_1_epo.fif) does not conform to MNE naming conventions. All events files should end with .eve, -eve.fif, -eve.fif.gz, -eve.lst, -eve.txt, _eve.fif, _eve.fif.gz, _eve.lst, _eve.txt or -annot.fif\n",
      "  imagination_events = mne.read_events(imagine_path+ datapoint)\n",
      "C:\\Users\\hlw69\\AppData\\Local\\Temp\\ipykernel_15448\\1644791318.py:17: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs([perception_epochs,imagination_epochs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "220 matching events found\n",
      "No baseline correction applied\n"
     ]
    }
   ],
   "source": [
    "# load up files for one subject for one task of img vs. perc\n",
    "# imagine_pictorial vs. perception_pictorial\n",
    "\n",
    "\n",
    "perception_path = 'X:\\\\CompSci\\\\ResearchProjects\\\\EJONeill\\\\Neuroimaging\\\\multisensoryeeg\\\\processed_eeg\\\\epochs\\\\perception_'+task+'\\\\'\n",
    "imagine_path = 'X:\\\\CompSci\\\\ResearchProjects\\\\EJONeill\\\\Neuroimaging\\\\multisensoryeeg\\\\processed_eeg\\\\epochs\\\\imagine_'+task+'\\\\'\n",
    "datapoint = subject+'_'+session+ '_epo.fif'\n",
    "\n",
    "perception_epochs = mne.read_epochs(perception_path + datapoint)\n",
    "perception_events = mne.read_events(perception_path+ datapoint)\n",
    "perception_epochs = perception_epochs.crop(tmin=0, tmax=duration)\n",
    "\n",
    "imagination_epochs = mne.read_epochs(imagine_path + datapoint)\n",
    "imagination_events = mne.read_events(imagine_path+ datapoint)\n",
    "imagination_epochs = imagination_epochs.crop(tmin=0, tmax=duration)\n",
    "epochs = mne.concatenate_epochs([perception_epochs,imagination_epochs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d523996",
   "metadata": {},
   "source": [
    "### Dividing Epochs into Perception and Classification\n",
    "We use the combine_event_ids to merge together all the perception epochs (for the three semantic categories), then do the same again for imagination. These two groups are renamed to be 'perception' and 'imagination'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "806a9038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perc_flower_p': 312, 'perc_penguin_p': 313, 'perc_guitar_p': 314, 'imag_flower_p': 303, 'imag_penguin_p': 304, 'imag_guitar_p': 305}\n"
     ]
    }
   ],
   "source": [
    "# Here we check that the epoch event ids are as expected\n",
    "print(epochs.event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5188185e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<EpochsArray |  220 events (all good), 0 â€“ 3 s, baseline off, ~639.8 MB, data loaded,\n",
      " 'perception': 109\n",
      " 'imagination': 111>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = mne.epochs.combine_event_ids(epochs, old_event_ids = ['perc_penguin_'+tag,'perc_flower_'+tag,'perc_guitar_'+tag],new_event_id = {'perception':1})\n",
    "epochs = mne.epochs.combine_event_ids(epochs, old_event_ids = ['imag_penguin_'+tag,'imag_flower_'+tag,'imag_guitar_'+tag], new_event_id = {'imagination':0})\n",
    "print(epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6a45b",
   "metadata": {},
   "source": [
    "### We create the labels for classification\n",
    "#### 1 = perception\n",
    "#### 0 = imagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0c323f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Shape of epoch data  (220, 124, 3073)\n"
     ]
    }
   ],
   "source": [
    "labels = epochs.events[:, -1]\n",
    "print(\"Labels\", labels)\n",
    "print(\"Shape of epoch data \", epochs.get_data().shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f63fd",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Here, stratified cross validation is implemented with 5 fold. 50 iterations are set. However, the amount of iterations can be edited in the parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a33fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier = [0.75       0.68181818 0.65909091 0.77272727 0.68181818]. Mean accuracy = 0.709090909090909\n",
      "Classifier = [0.75       0.68181818 0.65909091 0.77272727 0.68181818]. Mean accuracy = 0.709090909090909\n",
      "Classifier = [0.75       0.68181818 0.65909091 0.77272727 0.68181818]. Mean accuracy = 0.709090909090909\n",
      "Classifier = [0.75       0.68181818 0.65909091 0.77272727 0.68181818]. Mean accuracy = 0.709090909090909\n",
      "Classifier = [0.75       0.68181818 0.65909091 0.77272727 0.68181818]. Mean accuracy = 0.709090909090909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from mne.decoding import CSP, Scaler, Vectorizer, cross_val_multiscore,PSDEstimator,UnsupervisedSpatialFilter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "data = epochs.get_data()\n",
    "clf = make_pipeline(\n",
    "    Scaler(epochs.info),\n",
    "  #  psd,\n",
    " #   UnsupervisedSpatialFilter(PCA(124), average=False),  # this has to be done due to this error: https://github.com/mne-tools/mne-python/issues/9094\n",
    "#    csp,\n",
    "    Vectorizer(),\n",
    "    svm.SVC()\n",
    "  #  LogisticRegression(solver='liblinear')  # liblinear is faster than lbfgs\n",
    "  #AdaBoostClassifier(n_estimators=100)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "from random import randint # ensure that different random_state each time\n",
    "for i in range(iterations):\n",
    "    scores = cross_val_score(clf, data, labels, cv=5)\n",
    "    print(f\"Classifier = {scores}. Mean accuracy = {scores.mean()}\")\n",
    "    accuracies.append(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bafc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
